{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdgW1KcoVug5OYm8FKvz3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NadiaHolmlund/BDS_M6_Exam_Notes/blob/main/BDS_M6_Exam_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Databases"
      ],
      "metadata": {
        "id": "H9C25kaBkcyB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. What are the main types of databases and what are the main differences between them?"
      ],
      "metadata": {
        "id": "VjdlGQT7kcvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Relational databases:\n",
        " - Store data in tables with rows and columns.\n",
        " - Relational database use SQL (Structured Query Language) to manipulate data.\n",
        " - Well suited for structured data and are widely used in business applications.\n",
        "\n",
        "- NoSQL databases:\n",
        " - Use a document-based model, key-value pairs, column-family or graphs.\n",
        " - Often used for large and unstructured data sets\n",
        " - Can handle different types of data such as images, videos, and social media feeds.\n",
        "\n",
        "\n",
        "The main differences:\n",
        "- Data structures\n",
        "- The way they store and organize data\n",
        "- The way they handle queries and data access\n",
        " \n",
        "Main advantages:\n",
        "- Relational databases are well suited for structured data and support complex queries\n",
        "- NoSQL databases are well suited for unstructured data and are more scalable"
      ],
      "metadata": {
        "id": "pPu_rtkOUisU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. How can we use a database in an ML project, and can you give an example?"
      ],
      "metadata": {
        "id": "65IKfqCGR0lH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Databases can be used to store, manage, and preprocess large amounts of data, which can then be used for model training, testing, and validation.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "Use a database, such as MySQL or PostgreSQL, to store data in a structured format. Then use SQL queries to extract and transform the data into a format that can be used for training and testing a machine learning model. For example, we can use SQL queries to:\n",
        "- Join multiple tables to combine data from different sources\n",
        "- Filter and select data based on specific criteria\n",
        "- Aggregate data to calculate summary statistics\n",
        "- Normalize and preprocess data to prepare it for machine learning algorithms, such as scaling or one-hot encoding categorical features.\n",
        "- Once the data has been transformed and preprocessed, we can use it to train and test our machine learning model"
      ],
      "metadata": {
        "id": "lMcSXxH9Vsly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQL"
      ],
      "metadata": {
        "id": "olef9OlaRJmp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL database:\n",
        "* SQL stands for Structured Query Language\n",
        "* SQL is a domain-specific language for managing relational databases (tables with relationships)\n",
        "* CRUD operations: Create, Read, Update, and Delete data\n",
        "* Commonly used in data analytics, data science, and data engineering"
      ],
      "metadata": {
        "id": "OZ9juKDobsB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a relational database"
      ],
      "metadata": {
        "id": "NwUlhri1e2bH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Relational Databases \n",
        "\n",
        "* Relational database managament systems (RDBMS) store data in tables with rows and columns\n",
        "  * Each row represents a record or an entity\n",
        "  * Each column represents an attribute of the record\n",
        "* Tables are connected through relationships (primary and foreign keys)\n",
        "\n",
        "Example: \n",
        "\n",
        "| id | first_name | last_name | age | affiliation_id |\n",
        "|----|------------|-----------|-----|----------------|\n",
        "| 1  | John       | Doe       | 30  | 1337           |\n",
        "| 2  | Jane       | Smith     | 25  | 1505           |"
      ],
      "metadata": {
        "id": "rX177W0xbxUQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. What is a primary key in a relational database?"
      ],
      "metadata": {
        "id": "DQNQqvo4kctJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A primary key is a unique identifier for each record (row) in a table.\n",
        "- It is a column or combination of columns that uniquely identifies a record and ensures that each record in a table can be accessed and updated efficiently.\n",
        "- The primary key is typically used as a reference in other tables (foreign keys) to establish relationships between tables\n",
        "\n",
        "For example:\n",
        "\n",
        "In a customer and orders table, the customer ID column could be the primary key in the customer table and a foreign key in the orders table, linking the two tables together."
      ],
      "metadata": {
        "id": "R1dndBW3cZHV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. What is a foreign key in a relational database?"
      ],
      "metadata": {
        "id": "MI2QlCOfkcqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A foreign key is a column or a combination of columns that references the primary key of another table.\n",
        "- It is used to establish a relationship between two tables by creating a link between them.\n",
        "- Foreign keys ensure referential integrity by preventing actions that would violate relationships between tables, such as deleting a record that is referenced by a foreign key in another table.\n",
        "- They also allow for efficient retrieval of related data from multiple tables.\n",
        "\n",
        "For example:\n",
        "\n",
        "In a customer and orders table, the customer ID column could be the primary key in the customer table and a foreign key in the orders table, linking the two tables together. This allows the orders table to associate each order with a specific customer in the customer table."
      ],
      "metadata": {
        "id": "Ko-8RFH7cdUN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Can you give me some examples of SQL syntax (statements) and their applications?"
      ],
      "metadata": {
        "id": "k--WSeJ-kcnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SQL Syntax Overview:\n",
        "\n",
        "* SELECT: Read data from the database\n",
        "* INSERT: Add new records to the database\n",
        "* UPDATE: Modify existing records in the database\n",
        "* DELETE: Remove records from the database\n",
        "* CREATE, ALTER, DROP: Manage database structure\n",
        "* WHERE: Filter rows based on a condition\n",
        "* ORDER BY: Arrange rows based on a column\n",
        "* JOIN: Combine tables based on common columns\n",
        "* GROUP BY: operations on group level"
      ],
      "metadata": {
        "id": "ZZ3sYNx_b-ZA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Give some examples of SQL applications."
      ],
      "metadata": {
        "id": "gIyjfrbykclK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Analytics:\n",
        "* Data warehousing for storing historical data\n",
        "* Structured data analysis for business intelligence and reporting\n",
        "* Support for complex queries and aggregations for decision-making\n",
        "\n",
        "Machine Learning:\n",
        "* Data preprocessing for machine learning algorithms\n",
        "* Feature engineering to create relevant features from raw data\n",
        "* Joining multiple data sources for a comprehensive dataset\n",
        "* Storing and managing machine learning model metadata and results\n"
      ],
      "metadata": {
        "id": "cKzPCELKb648"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. What are the different types of SQL and what are their key features and use cases?"
      ],
      "metadata": {
        "id": "rsYPzdzskcig"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types of SQL databases:\n",
        "\n",
        "- MySQL\n",
        " - MySQL is one of the most popular fully-managed database types in SQL-based management.\n",
        "\n",
        "- PostgreSQL\n",
        " - PostgreSQL is an advanced type of database in SQL management systems that seeks to step up MySQL solutions.\n",
        " - PostgreSQL blends the traditional table-based approach with user-defined objects to create resilient databases supporting and analyzing complex and voluminous data.\n",
        "\n",
        "- SQLite\n",
        " - SQLite is a type of SQL database or storage engine structurally considered equivalent to a C library. It is embedded within other applications to enhance their storage capabilities. It is often used as the on-disk file format in applications for financial analysis, cataloging, etc.\n",
        "\n",
        "- Microsoft SQL Server\n",
        " - Microsoft SQL Server (MSSQL) is one of the most popular DBMS in SQL for innovative management solutions. T-SQL, a derivative of SQL, is used to interact with MSSQL databases. The 2019 version of MSSQL comes integrated with Apache Spark and Hadoop Distributed File System for big data management and analysis.\n",
        "\n",
        "- MariaDB\n",
        " - MariaDB is an open-source fork of MySQL. It intends to remain freely accessible to all under the General Public License. It is a database management system in SQL that seeks to be an alternative to MySQL DBMS.\n",
        "\n",
        "- Oracle\n",
        " - The relational database management system provided by Oracle Corp. is a multi-model RDBMS that can support diverse, multiple workloads. This DBMS type in SQL is commonly used for online transaction processing and data warehousing."
      ],
      "metadata": {
        "id": "hODoHabbcN1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. What are the main advantages of SQL?"
      ],
      "metadata": {
        "id": "6OG4haCXkca2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of SQL:\n",
        "- ACID transactions:\n",
        " - Atomicity: transactions are all or nothing, meaning that either all the changes in the transaction are committed, or none of them are\n",
        " - Consistency: transactions takes the database from one valid state to another, and that any constraints or rules in place are not violated during the transaction.\n",
        " - Isolation: transactions are executed in isolation from other transactions, so that they don't interfere with each other\n",
        " - Durability): once a transaction is committed, it will persist even in the face of power failures, system crashes, or other failures\n",
        "\n",
        "- Standardized language:\n",
        " - SQL for querying and managing data\n",
        "\n",
        "- High Performance:\n",
        " - designed to handle large volumes of data efficiently and quickly process complex queries.\n",
        "\n",
        "- Scalability:\n",
        " - can scale to accommodate increasing amounts of data and users.\n",
        "\n",
        "- Flexibility:\n",
        " - allows for the manipulation of data in various ways, incl. sorting, filtering, and aggregating\n",
        "\n",
        "- Security:\n",
        " - offer built-in security features that allow for controlled access to data.\n",
        "\n",
        "- Data Integrity:\n",
        " - ensure data integrity through the use of constraints, such as unique keys, foreign keys, and check constraints.\n",
        "\n",
        "- Compatibility: \n",
        " - supported by many database management systems, making it a widely adopted and compatible language for data storage and manipulation."
      ],
      "metadata": {
        "id": "5Y2OAzVYb3V7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NoSQL"
      ],
      "metadata": {
        "id": "5OH01O9PRK1e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NoSQL Databases:\n",
        "* NoSQL stands for \"Not only SQL\"\n",
        "* Non-relational databases (document, key-value, column-family, graph)\n",
        "* Designed to handle unstructured data and scalability challenges\n",
        "* Flexible schema allows for evolving data structures\n",
        "* Horizontal scalability for handling large volumes of data\n",
        "* Generally, weaker consistency models compared to SQL databases"
      ],
      "metadata": {
        "id": "7Wf5oa1fbuK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can you give me some examples of NoSQL syntax (statements) and their applications?"
      ],
      "metadata": {
        "id": "0V9QFdC2VUCo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NoSQL databases are often schema-less, which means that they don't have a fixed structure for storing data like SQL databases. Instead, they use various query languages and data models to manipulate data."
      ],
      "metadata": {
        "id": "cDl9AvMudq8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. What are the different types of NoSQL and what are their key features and use cases?"
      ],
      "metadata": {
        "id": "kp1pJ8Cqkcfv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Types and Structure of NoSQL Databases\n",
        "\n",
        "- Document Databases\n",
        "  * Store data in documents (usually JSON or BSON)\n",
        "  * Documents can have nested structures\n",
        "  * Documents can be organized in collections\n",
        "  * Examples: MongoDB, Couchbase, RavenDB\n",
        "  - Applications:\n",
        "    * Content management systems (CMS) with complex data structures\n",
        "    * Real-time analytics for handling unstructured data\n",
        "    * IoT data management for handling diverse data from multiple devices"
      ],
      "metadata": {
        "id": "Nv27e07z4nz3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Key-Value Stores\n",
        " * Store data as key-value pairs\n",
        " * Keys are unique identifiers for the data\n",
        " * Values can be any data type, including complex structures\n",
        " * Examples: Redis, Amazon DynamoDB, Riak KV\n",
        " - Applications:\n",
        "   * Caching for improving the performance of data retrieval\n",
        "   * Session management for storing user-specific data across sessions\n",
        "   * Configuration data storage for application settings and metadata"
      ],
      "metadata": {
        "id": "AS-L5KTd4wPf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Column-Family Stores\n",
        " * Store data in column families (loosely related to tables)\n",
        " * Data is organized as rows and columns\n",
        " * Designed for high write and read performance on large-scale data\n",
        " * Examples: Apache Cassandra, HBase, ScyllaDB\n",
        " Applications:\n",
        "   * Time-series data management for high write and read workloads\n",
        "   * Event logging and analytics for large-scale systems\n",
        "   * Recommendation systems for analyzing user behavior and preferences"
      ],
      "metadata": {
        "id": "0QOy3Phv5e8C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Graph Databases\n",
        " * Store data as nodes (entities) and edges (relationships)\n",
        " * Designed for graph traversal and querying connected data\n",
        " * Examples: Neo4j, Amazon Neptune, ArangoDB\n",
        " - Applications:\n",
        "   * Social network analysis for exploring connections between users\n",
        "   * Fraud detection for uncovering complex patterns in financial transactions\n",
        "   * Knowledge graphs for storing and querying complex, interrelated information"
      ],
      "metadata": {
        "id": "wb1xESSEdYSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. What are the main advantages of NoSQL?"
      ],
      "metadata": {
        "id": "ZloInQStkcYK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Advantages of NoSQL Databases:\n",
        "* Flexibility:\n",
        " - Schema flexibility for changing data requirements to handle unstructured data\n",
        "* Scalability/performance:\n",
        " - High scalability for large data volumes and high read/write rates\n",
        "* Availability:\n",
        " - High availability and fault tolerance through data replication even if a node cluster fails\n",
        "* Schemaless:\n",
        " - Optimized for specific use cases, depending on the type of NoSQL database\n",
        "- Cost:\n",
        " - NoSQL databases can be less expensive than traditional SQL databases"
      ],
      "metadata": {
        "id": "jkSrNly9dTUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Big Data"
      ],
      "metadata": {
        "id": "YSYk_f9jRMnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. What is Apache Spark used for?"
      ],
      "metadata": {
        "id": "TgaD05TjkcVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Apache Spark is an open-source distributed computing system that provides an interface for programming entire clusters with implicit data parallelism and fault tolerance. It is designed to handle large datasets by distributing the data and computation across a cluster of computers\n",
        "- Apache Spark is used for large-scale data processing and analytics.\n",
        "- It is designed to handle batch processing, streaming data processing, machine learning, graph processing, and other data processing workloads.\n",
        "- One of the main advantages of Spark is its ability to run SQL commands for data analysis. \n",
        "\n",
        "Common use cases:\n",
        "- Data processing:\n",
        " - Process large volumes of data quickly and efficiently.\n",
        " - Supports structured, semi-structured, and unstructured data.\n",
        "\n",
        "- Machine learning:\n",
        " - Built-in libraries classification, regression, clustering, and collaborative filtering.\n",
        "\n",
        "- Real-time analytics:\n",
        " - Spark's streaming API enables real-time data processing and analytics.\n",
        "\n",
        "- Graph processing:\n",
        " - Spark's graph processing API enables the processing and analysis of large-scale graph data.\n",
        "\n",
        "- Data integration:\n",
        " - Spark can be used to integrate data from multiple sources and transform it into a common format for analysis.\n",
        " - This makes it useful for data warehousing and ETL (extract, transform, load) processes."
      ],
      "metadata": {
        "id": "onPSVQ7IfLsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Can you provide an example of a use case where Apache Spark might be preferred?"
      ],
      "metadata": {
        "id": "N09yCNlLkcTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example: e-commerce website that generates a massive amount of data, including customer interactions, clickstream data, and purchase history\n",
        "\n",
        "Task: analyze data to gain insights into customer behavior, optimize marketing campaigns, and improve the customer experience.\n",
        "\n",
        "- Real-time analytics (Spark streaming API):\n",
        " - Analyze customer interactions and clickstream data in real-time, to optimize customer experience and improve conversion rates.\n",
        "\n",
        "- Machine learning (built in libraries):\n",
        " - Build and train models on large datasets to personalize the customer experience, recommend products, and optimize marketing campaigns.\n",
        "\n",
        "- Data integration:\n",
        " - Integrate data from multiple sources, including social media, email marketing, and customer service, into a single platform for analysis to gain a comprehensive view of customer behavior and preferences.\n",
        "\n",
        "- Data processing:\n",
        " - Process and clean up large volumes of data quickly and efficiently, making it easier to prepare data for analysis."
      ],
      "metadata": {
        "id": "xlx-6I9agZ6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Polars used for?"
      ],
      "metadata": {
        "id": "UTAxyrHNUtRJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Polars and Why is it Faster Than Pandas?\n",
        "\n",
        "Polars is a DataFrame library designed for parallelization. It is built from the ground up and written in Rust but also has a Python package, making it a potential alternative to Pandas. \n",
        "> Polars has two different APIs: an eager API and a lazy API. Eager execution is similar to Pandas, while lazy execution is more efficient because it avoids running unnecessary code. \n",
        "\n",
        "Polars is faster than Pandas because it utilizes all available cores on your machine. Polars has different syntax from Pandas and can perform operations in parallel. However, Polars code is usually a little longer than the Pandas code. If you need to do a lot of data processing on large datasets, Polars can be a good alternative to Pandas."
      ],
      "metadata": {
        "id": "fzbxlTbSi235"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Polars?\n",
        "The best way to understand Polars is that it is a better dataframe library than Pandas. Here are some advantages of Polars over Pandas:\n",
        "\n",
        "- Polars does not use an index for the dataframe. Eliminating the index makes it much easier to manipulate the dataframe (the index is mostly redundant in Pandas dataframe anyway).\n",
        "- Polars represents data internally using Apache Arrow arrays while Pandas stores data internally using NumPy arrays. Apache Arrow arrays is much more efficient in areas like load time, memory usage, and computation.\n",
        "- Polars supports more parallel operations than Pandas. As Polars is written in Rust, it can run many operations in parallel.\n",
        "- Polars supports lazy evaluation. Based on your query, Polars will examine your queries, optimize them, and look for ways to accelerate the query or reduce memory usage. Pandas, on the other hand, support only eager evaluation, which immediately evaluates an expression as soon as it encounters one."
      ],
      "metadata": {
        "id": "QuIIc2OTjTTj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Can you provide an example of a use case where Polars might be preferred?"
      ],
      "metadata": {
        "id": "H1kkbqoiU9ej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. What are the key differences between Apache Spark, Polars, and Pandas, and their use cases?"
      ],
      "metadata": {
        "id": "b8NT0Ok7kcQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison between Pandas and Polars\n",
        "At first glance, Pandas and Polars (eager API) are similar regarding syntax because of their shared main building blocks: Series and DataFrames."
      ],
      "metadata": {
        "id": "M75fk-MMi9oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apache Spark, Polars, and Pandas are all popular data processing and analytics tools, but they have some key differences in terms of their design, features, and use cases. Here's a comparison of the three:\n",
        "\n",
        "- Apache Spark: Apache Spark is a distributed computing system that is designed to process large datasets quickly and efficiently. It includes a range of libraries for batch processing, streaming data processing, machine learning, and graph processing. Spark's key features include scalability, fault tolerance, and real-time data processing. It is best suited for processing and analyzing very large datasets, where distributed processing is necessary for performance.\n",
        "- Polars: Polars is a data processing library for Python and Rust that is designed to provide fast, memory-efficient data processing capabilities for large datasets. It includes a range of data manipulation and aggregation functions, as well as support for parallel processing and GPU acceleration. Polars is best suited for working with large, structured datasets that require complex data transformations and filtering.\n",
        "- Pandas: Pandas is a popular data manipulation library for Python that is designed for working with smaller datasets. It includes a range of functions for data cleaning, transformation, and aggregation, as well as support for data visualization. Pandas is best suited for working with structured data that can fit into memory on a single machine.\n",
        "\n",
        "Overall, the key differences between these tools come down to their scalability and performance capabilities. Apache Spark is designed for processing and analyzing very large datasets that require distributed processing, while Polars is designed for fast, memory-efficient processing of large structured datasets. Pandas is designed for working with smaller datasets that can fit into memory on a single machine. The choice of tool depends on the size and complexity of the dataset, the performance requirements, and the specific use case."
      ],
      "metadata": {
        "id": "XM4qKwIejjD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLOps"
      ],
      "metadata": {
        "id": "oS9Zd72JkcN4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 14. What is MLOps, and why is it important in the context of machine learning projects?"
      ],
      "metadata": {
        "id": "AajusZZTkcLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition:\n",
        "- MLOps is the process of taking experimental ML models into a production system.\n",
        "- MLOps (Machine Learning Operations) is a set of practices and tools used to manage the lifecycle of ML models in production environments. It combines principles and techniques from ML, DevOps, and data engineering to create a consistent and efficient process for managing ML projects.\n",
        "\n",
        "![picture](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/ML_Ops_Venn_Diagram.svg.png)"
      ],
      "metadata": {
        "id": "ps3lWH1JAPE6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is MLOps important:\n",
        "\n",
        "- Collaboration:\n",
        " - MLOps provides a common framework for collaboration and communication across teams, ensuring that everyone is working towards the same goals.\n",
        "\n",
        "- Efficiency:\n",
        " - MLOps helps to automate many of the repetitive tasks involved in ML projects, such as data cleaning, feature engineering, model training, and deployment.\n",
        "\n",
        "- Reproducibility:\n",
        " - MLOps provides a systematic approach to managing ML projects, including version control, testing, and documentation.\n",
        "\n",
        "- Scalability:\n",
        " - MLOps provides tools and techniques for managing data, models, and infrastructure at scale, allowing teams to deploy and manage models across multiple environments.\n",
        "\n",
        "\n",
        "![pciture](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/HIddenTechnicalDebtinML.jpg)"
      ],
      "metadata": {
        "id": "VOSbr-XpCqN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary:\n",
        "\n",
        "MLOps is important because it helps to streamline the development, deployment, and maintenance of models, making it easier to collaborate across teams, improve efficiency, ensure reproducibility, and scale up projects as needed.\n",
        "\n",
        "Stats:\n",
        "- ~85 % of ML models that are built never reach production\n",
        "- ~60% of projects make it from prototype to production"
      ],
      "metadata": {
        "id": "TkASanXcCnG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 15. How does MLOps help in streamlining the machine learning lifecycle from development to deployment? Answer based on an example."
      ],
      "metadata": {
        "id": "CR6pd_pxkcI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLOps helps to streamline the machine learning lifecycle from development to deployment by providing a systematic approach to managing machine learning projects. For example:\n",
        "\n",
        "- Data preparation:\n",
        " - MLOps provides tools and techniques for managing data, including data cleaning, transformation, and feature engineering.\n",
        " - For example, data pipelines can be developed and managed using tools such as Apache Airflow or Kubeflow, allowing data to be processed and transformed automatically.\n",
        "\n",
        "- Model development:\n",
        " - MLOps provides tools and techniques for managing the model development process, including version control, testing, and documentation.\n",
        " - For example, models can be developed and tested using tools such as Jupyter notebooks or PyCharm, and code can be managed using version control systems such as Git.\n",
        "\n",
        "- Model training:\n",
        " - MLOps provides tools and techniques for managing the model training process, including scalability and reproducibility.\n",
        " - For example, models can be trained using distributed computing frameworks such as Apache Spark or TensorFlow, and training can be managed using tools such as Kubeflow or MLflow.\n",
        "\n",
        "- Model deployment:\n",
        " - MLOps provides tools and techniques for managing the model deployment process, including automation and scalability.\n",
        " - For example, models can be deployed using containerization technologies such as Docker or Kubernetes, and deployment can be managed using tools such as GitOps or Jenkins.\n",
        "\n",
        "- Model monitoring:\n",
        " - MLOps provides tools and techniques for managing the model monitoring process, including tracking model performance and detecting anomalies.\n",
        " - For example, models can be monitored using tools such as Prometheus or Grafana, and alerts can be triggered based on specific criteria."
      ],
      "metadata": {
        "id": "0xOAWtg_EJBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 16. How do Continuous Integration (CI) and Continuous Deployment (CD) principles apply to MLOps, and can you give a use case?"
      ],
      "metadata": {
        "id": "dL_Oko5tkcGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitions:\n",
        "\n",
        "- Continuous Integration (CI) is the practice of regularly integrating code changes into a shared repository, and automatically building and testing the code to identify and fix any issues.\n",
        "- Continuous Deployment (CD) is the practice of automatically deploying code changes to production once they have been tested and verified.\n",
        "\n",
        "We can also consider other \"continuous' concepts\":\n",
        "\n",
        "- Continuous Training (CT): Increasing automation allows a model to be retrained when new data becomes available. \n",
        "- Continuous Monitoring (CM): Another reason to retrain a model is decreasing performance. We should also understand whether models are still delivering value against business metrics. \n",
        "\n",
        "CI/CD can help streamline the ML development process, by automating tasks involved in building, testing, and deploying models. For example:\n",
        "\n",
        "- Continuous Integration:\n",
        " - Data scientists can use CI tools such as GitHub Actions, CircleCI, or Jenkins to automatically build and test their models whenever changes are made to the code or data.\n",
        " - For example, when a new feature is added to the model or when new data becomes available, the CI system can automatically rebuild and test the model to ensure that it is still working as expected.\n",
        "\n",
        "- Continuous Deployment:\n",
        " - Once a model has been developed and tested, CD tools such as Kubernetes, Argo, or Jenkins can be used to automatically deploy the model to production.\n",
        " - For example, when a new version of the model is ready, the CD system can automatically deploy it to a testing environment, where it can be further evaluated and tested. Once the model has passed all tests, it can be automatically deployed to production."
      ],
      "metadata": {
        "id": "ZgSofurTHHv6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://www.synopsys.com/glossary/what-is-cicd/_jcr_content/root/synopsyscontainer/column_1946395452_co/colRight/image_copy.coreimg.svg/1663683682045/cicd.svg)"
      ],
      "metadata": {
        "id": "EDkBz5biFlrm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17. What are some popular tools and platforms used in MLOps for managing and deploying machine learning models?"
      ],
      "metadata": {
        "id": "cOmjHjz4kcD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Experiment Tracking and Model Metadata Management Tools:\n",
        " - MLFlow\n",
        " - Comet ML\n",
        " - Weights & Biases\n",
        "\n",
        "- Orchestration and Workflow Pipelines MLOps Tools:\n",
        " - Prefect\n",
        " - Metaflow\n",
        " - Kedro\n",
        "\n",
        "- Data and Pipeline Versioning Tools:\n",
        " - Pachyderm\n",
        " - Data Version Control (DVC)\n",
        "\n",
        "- Model Deployment and Serving Tools:\n",
        " - TensorFlow Extended (TFX) Serving\n",
        " - BentoML\n",
        " - Cortex\n",
        "\n",
        "- Model Monitoring in Production ML Ops Tools:\n",
        " - Evidently\n",
        " - Fiddler\n",
        " - Censius AI\n",
        "\n",
        "- End-to-End MLOps Platforms\n",
        " - AWS SageMaker\n",
        " - DagsHub\n",
        " - Kubeflow"
      ],
      "metadata": {
        "id": "eAYTUPpcNWul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 18. Can you give a best practice for implementing MLOps in an organization?"
      ],
      "metadata": {
        "id": "KiIH1U4ckcBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow a systematic and iterative approach that involves the following steps:\n",
        "\n",
        "- Define your ML goals and use cases:\n",
        " - Identify business problems that you want to solve using machine learning.\n",
        " - Define the specific ML use cases and goals that you want to achieve.\n",
        "\n",
        "- Assemble a team:\n",
        " - Build a team with the necessary skills to work on the project.\n",
        "\n",
        "- Define data pipelines:\n",
        " - Create data pipelines to collect, preprocess, and store data for machine learning.\n",
        " - Use version control systems to manage code and data.\n",
        "\n",
        "- Develop and train models:\n",
        " - Develop models using best practices in data science and machine learning.\n",
        " - Use scalable architectures to build models that can handle large datasets.\n",
        "\n",
        "- Test and evaluate models:\n",
        " - Test models using validation and testing techniques to ensure that they are accurate and reliable.\n",
        " - Establish a process to monitor models in production and to retrain models when necessary.\n",
        "\n",
        "- Deploy models:\n",
        " - Deploy models using containerization tools like Docker or Kubernetes.\n",
        " - Establish a process for continuous integration and continuous deployment (CI/CD) to streamline the deployment process.\n",
        "\n",
        "- Monitor and optimize:\n",
        " - Establish a process to monitor the performance of models in production and to optimize them when necessary."
      ],
      "metadata": {
        "id": "nxFaVT_3PFIu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLFlow"
      ],
      "metadata": {
        "id": "XpZa3TjjlaQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 19. What is MLflow, and how can it help in terms of MLOps?"
      ],
      "metadata": {
        "id": "LoAFFXNwkb_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- MLFlow is a platform for managing the end-to-end machine learning lifecycle\n",
        "- In terms of MLOps, MLFlow provides a centralized platform for managing the ML lifecycle, from data preparation and model development to deployment and monitoring. Using MLflow, teams can work collaboratively on ML projects, easily reproduce experiments, and deploy models in a standardized and scalable way.\n",
        "\n",
        "- It provides tools for:\n",
        " - Tracking experiments\n",
        " - Packaging and sharing code\n",
        " - Deploying models\n",
        "\n",
        "- It helps to increase:\n",
        " - Productivity\n",
        " - Collaboration\n",
        " - Reproducibility"
      ],
      "metadata": {
        "id": "g8hG6uvpwpHg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main components of MLflow:\n",
        " - Experiment Tracking:\n",
        "   - Helps track ML experiments by recording and visualizing:\n",
        "    - Metrics: values computed during an experiment, e.g. accuracy, loss, F1 score\n",
        "    - Parameters: configuration used during an experiment, e.g. learning rate, batch size, epochs\n",
        "    - Artifacts: outputs or results of an experiment, e.g. trained models, viz or data files\n",
        "   - Allows easy comparison of experiment runs and reproducable results\n",
        "\n",
        "- Model Packaging:\n",
        "    - Simple format for packaging code in a reusable and reproducible way\n",
        "    - Allows to specify dependencies, such as libraries and data files, and to run code in different environments\n",
        "    - Provides a standardized way to package and deploy machine learning models, supporting frameworks, such as TensorFlow, PyTorch, and Scikit-learn, and tools for deploying models, such as Docker containers and cloud services\n",
        "\n",
        "- Model Registry:\n",
        "   - Centralized repository for managing and sharing ML models\n",
        "   - Allows to track model versions, assign permissions, and share models with other users\n"
      ],
      "metadata": {
        "id": "TckLYyyMyGdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://raw.githubusercontent.com/aaubs/ds-master/main/data/Images/mlflow.jpg)"
      ],
      "metadata": {
        "id": "jZqCCPdryCMZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 20. What types of information can be stored in MLflow?"
      ],
      "metadata": {
        "id": "NGSZ_0gJkb8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLflow can store information related to ML experiments, including:\n",
        "- Metrics: Numeric values that track the performance of a model during training or testing, such as accuracy, precision, recall, and F1-score.\n",
        "- Parameters: Settings and configurations used during model training, such as learning rate, batch size, and number of epochs.\n",
        "- Artifacts: Any output generated during model training or evaluation, such as trained models, visualizations, or data preprocessing scripts.\n",
        "- Source code: The code used to train and evaluate the model, along with any dependencies and environment settings. Experiment metadata: Information about the experiment, such as the name, date, user, and any tags or annotations.\n",
        "\n",
        "The information is stored in a centralized repository, which can be accessed and managed through the MLflow UI or API."
      ],
      "metadata": {
        "id": "El67zZND3cxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SSH"
      ],
      "metadata": {
        "id": "j9Q3Qmmdkb53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 21. What is the purpose of SSH?"
      ],
      "metadata": {
        "id": "nLVL7Mbqkb3b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SSH = Secure Shell\n",
        "- Provides a secure connection between two entities (computer/computer or computer/remote server) over an unsecured network such as the internet\n",
        "- Uses encryption and public-key authentication to protect sensitive information and data transferred between the two entities\n",
        "- E.g. in terminal: ssh ubuntu@Public_IP -i key.pem     \n",
        " - ssh: command to start an SSH session\n",
        " - ubuntu: Ubuntu operating system images provided by AWS\n",
        " - Public_IP: public IP address of the AWS instance you want to connect to\n",
        " - -i key.pem: the path to the private key file downloaded from AWS. The key authenticates the connection to the AWS instance."
      ],
      "metadata": {
        "id": "FfhjPPGJmvke"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Docker"
      ],
      "metadata": {
        "id": "gEoD02Wvkb0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 22. What is Docker, and why is it useful?"
      ],
      "metadata": {
        "id": "tHghfR_6kbyW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Docker is an open-source containerization platform that allows software developers and system administrators to package, distribute, and run applications in isolated and portable containers.\n",
        "- Containers are lightweight, standalone, and portable units that contain all the necessary dependencies and configurations needed to run an application.\n",
        "\n",
        "Docker is useful for several reasons:\n",
        "\n",
        "- Portability: Docker allows developers to package an application with all its dependencies and configurations into a container, making it easily portable and deployable across different environments, such as local machines, servers, and cloud platforms.\n",
        "- Consistency: By using Docker, developers can ensure that the application runs consistently across different environments, eliminating the risk of issues arising due to differences in the underlying infrastructure or dependencies.\n",
        "- Scalability: Docker enables developers to quickly and easily spin up multiple instances of an application in a matter of seconds, making it easier to scale applications to meet changing demand.\n",
        "- Isolation: Docker containers provide a secure and isolated runtime environment, which helps prevent conflicts between different applications and dependencies.\n",
        "- Efficiency: Docker containers are lightweight and have minimal overhead, making them faster and more efficient than traditional virtual machines.\n",
        "\n",
        "Overall, Docker is a powerful tool that simplifies the process of building, distributing, and running applications, enabling developers to focus on their core tasks rather than the underlying infrastructure.\n",
        "\n",
        "![picture](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*gLwtHvsO8yebQzwB05nZ8Q.png)"
      ],
      "metadata": {
        "id": "cNLW-x-YKJxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deploying Scalable ML Models:\n",
        "\n",
        "Docker can be combined with orchestration tools like Kubernetes or Docker Swarm to deploy scalable ML models. These tools help manage, scale, and distribute Docker containers across multiple nodes or clusters. This allows you to handle high loads and serve multiple requests concurrently, ensuring a reliable and efficient service.\n",
        "\n",
        "![picture](https://editor.analyticsvidhya.com/uploads/85227PSLLpU1LQX8EY9LNae5tvSpq0BXn7DLhlI9VRp-rMxPxtqcbwa6EpAeQI6WFheKQZ4jtvJC2DgaSW9Ogs3ON5BksIKFgxNlczWKTrCI8k0WrBFMA2byFJElr3V-tfLDSV0C1eRE6.png)"
      ],
      "metadata": {
        "id": "Yg5EWK8pLif1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 23. What are containers, and how do they relate to Docker?"
      ],
      "metadata": {
        "id": "AmzJ4wl3kbwB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Containers:\n",
        "\n",
        "- Containers are a technology that allows developers to package and run applications with all their dependencies and configurations in an isolated environment.\n",
        "- Containers are lightweight, portable, and provide a consistent runtime environment for applications, regardless of the underlying infrastructure."
      ],
      "metadata": {
        "id": "Q2snJHyvOHC4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Containers in relation to Docker:\n",
        "\n",
        "- Docker uses containers to package and distribute applications.\n",
        "- A Docker container is a running instance of a Docker image.\n",
        "- When a Docker image is run, it creates a container that runs in isolation from the host operating system, providing a secure, reproducible and consistent runtime environment for the application.\n",
        "- Multiple containers can be run on a single host, each with its own set of resources and isolated from other containers running on the same host.\n",
        "- Containers can be started, stopped, and removed, while the underlying Docker image remains unchanged."
      ],
      "metadata": {
        "id": "G435ebjVQVn_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 24. What is the difference between a virtual machine and a Docker container?"
      ],
      "metadata": {
        "id": "x8PAzHYdkbtZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference between a virtual machine (VM) and a Docker container is the way they use and interact with the host operating system.\n",
        " - Virtual machines are like a computer within a computer. They run a complete operating system and require a significant amount of resources to run (CPU, memory, disk space). Each VM is isolated from the host operating system and other VMs running on the same hardware.\n",
        " - Docker containers are like a lightweight wrapper around an application and its dependencies. They share the same kernel as the host operating system and are isolated from other containers running on the same hardware."
      ],
      "metadata": {
        "id": "5T0nIAeOKtMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Key differences include:\n",
        "\n",
        "- Architecture:\n",
        " - A virtual machine emulates an entire computer system, including the operating system, hardware, and virtualized resources\n",
        " - A Docker container shares the same kernel as the host operating system, but isolates the application and its dependencies from the rest of the system.\n",
        "\n",
        "- Resource usage:\n",
        " - Virtual machines require a significant amount of resources, including CPU, memory, and disk space, to run a complete operating system.\n",
        " - Docker containers are lightweight and share resources with the host operating system, which allows them to be more efficient and scalable.\n",
        "\n",
        "- Portability:\n",
        " - Virtual machines are more difficult to move between different virtualization platforms and operating systems.\n",
        " - Docker containers are designed to be highly portable and easily moved between different systems and environments.\n",
        "\n",
        "- Deployment speed:\n",
        " - Virtual machines can take longer to start up and shut down.\n",
        " - Docker containers can be started and stopped very quickly, allowing for rapid deployment and scaling of applications. \n",
        "\n",
        "In summary, virtual machines provide a complete virtualized system, while Docker containers provide a lightweight, isolated environment for running applications.\n",
        "\n",
        "Containers are more efficient, portable, and scalable than virtual machines, which makes them a popular choice for deploying modern applications.\n",
        "\n",
        "\n",
        "![](https://thesolving.com/wp-content/uploads/2021/05/How-docker-works-structure-and-functioning-in-detail.png)"
      ],
      "metadata": {
        "id": "atQSHT4oTC8X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 25. What is a Docker image, and how is it created?"
      ],
      "metadata": {
        "id": "gYtb2hebkbrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker image:\n",
        "- A Docker image is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings.\n",
        "- Images act as a blueprint or template from which containers can be created."
      ],
      "metadata": {
        "id": "Igf266ESPsrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How is it created:\n",
        "- Docker images are created using a Dockerfile, which is a text file that contains instructions on how to build the image.\n",
        "- The Dockerfile specifies the base image, sets environment variables, copies files, installs dependencies, and runs commands to configure the software.\n",
        "- The Dockerfile is then used to build the Docker image, which is stored in a Docker registry."
      ],
      "metadata": {
        "id": "8r1_gaMcVeAP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a simple Dockerfile that creates a Docker image for a Python Flask web application:\n",
        "- The Dockerfile sets the base image to Python 3.9\n",
        "- Installs the dependencies from the requirements file\n",
        "- Copies the application code to the working directory\n",
        "- Sets the environment variables\n",
        "- Exposes port 5000 for the application\n",
        "- Runs the Flask web server"
      ],
      "metadata": {
        "id": "nrqymD8wV7f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base image\n",
        "FROM python:3.9\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements file\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install the dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the application code\n",
        "COPY . .\n",
        "\n",
        "# Set the environment variables\n",
        "ENV FLASK_APP=app.py\n",
        "\n",
        "# Expose the application port\n",
        "EXPOSE 5000\n",
        "\n",
        "# Run the application\n",
        "CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]"
      ],
      "metadata": {
        "id": "bJkUes-0VvMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build the Docker image from the Dockerfile, run the command below in the same directory as the Dockerfile.\n",
        "\n",
        "This command builds a Docker image with the tag \"my-flask-app\" using the Dockerfile in the current directory (\".\") as the build context. Once the Docker image is built, it can be run as a Docker container on any platform that supports Docker."
      ],
      "metadata": {
        "id": "u8fIoCqPxp40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker build -t my-flask-app ."
      ],
      "metadata": {
        "id": "xgbN10FHxwMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 26. How do you run a Docker container from an image?"
      ],
      "metadata": {
        "id": "JrtBuBr6kbob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To run a Docker container from an image, run the following commands:"
      ],
      "metadata": {
        "id": "Ka4Pnf9Nhd8m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic syntax\n",
        "docker run <image_name>\n",
        "\n",
        "# Example from above\n",
        "docker run my-flask-app"
      ],
      "metadata": {
        "id": "lGUDbRDphfNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By default, Docker will create a new container from the image and start it in the foreground. You can interact with the container by typing commands into the terminal.\n",
        "\n",
        "However, you may want to customize the behavior of the container by passing additional arguments to the docker run command. Here are some common options:\n",
        "\n",
        "- -d: Runs the container in the background (detached mode).\n",
        "- -p: Maps a port on the host machine to a port on the container.\n",
        "- -name: Assigns a name to the container.\n",
        "- -v: Mounts a volume from the host machine to the container.\n",
        "- -e: Sets environment variables for the container.\n",
        "\n",
        "For example, to run the \"my-flask-app\" container in detached mode and map port 5000 on the host machine to port 5000 on the container, you can run the following command:"
      ],
      "metadata": {
        "id": "EW1befz4hmYe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docker run -d -p 5000:5000 my-flask-app"
      ],
      "metadata": {
        "id": "5hDkckb7hl24"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 27. What is Docker Hub, and what is its purpose?"
      ],
      "metadata": {
        "id": "2vavJNFHkbmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Docker Hub:\n",
        "- Public registry that allows developers to store, share, and distribute Docker images.\n",
        "- It serves as a central repository for Docker images (like GitHub does for code)."
      ],
      "metadata": {
        "id": "dH6r1cJ2zaIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose of Docker Hub:\n",
        "- To store Docker images and make them available to others.\n",
        "- Discover and download images that others have created and shared.\n",
        "- Collaborate with others by sharing images and collaborating on Dockerfiles.\n",
        "- Automate the build and testing of images using Continuous Integration (CI) tools.\n",
        "- Docker Hub also provides additional features for paid users, such as:\n",
        " - Private repositories\n",
        " - Role-based access control\n",
        " - Image vulnerability scanning"
      ],
      "metadata": {
        "id": "QLx4OvAbz6Hp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 28. What is a Dockerfile, and what are its main components?"
      ],
      "metadata": {
        "id": "InALGoN6kbj9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dockerfile:\n",
        "- A text file containing instructions and commands for building a Docker image.\n",
        "- It defines the environment, packages, dependencies, configurations, files and commands required to run the application.\n"
      ],
      "metadata": {
        "id": "jawjC2800g6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main components:\n",
        "- Base image:\n",
        " - This is the starting point for the image. The base image provides a set of pre-configured operating system files and packages that the new image can build on. \n",
        " - E.g. Base image: FROM python:3.9-alpine\n",
        "\n",
        "- Instructions:\n",
        " - These are the commands that tell Docker how to build the image. For example, an instruction might install a specific package or copy files from the local file system into the image\n",
        " - E.g. Instructions: RUN apk add --update git\n",
        "\n",
        "- Arguments:\n",
        " - These are variables that can be passed to the Dockerfile at build time.\n",
        " - Arguments allow the Dockerfile to be customized for different environments or use cases.\n",
        " - E.g. Arguments: ARG version=1.0\n",
        "\n",
        "- Environment variables:\n",
        " - These are variables that can be set within the image and used by the container at runtime.\n",
        " - E.g. Environment variables: ENV FLASK_APP=app.py\n",
        "\n",
        "- Labels:\n",
        " - These are key-value pairs that provide metadata about the image, such as the version, author, and build date.\n",
        " - E.g. Labels: LABEL maintainer=\"John Smith\"\n",
        "\n",
        "- Working directory:\n",
        " - This is the directory where the Dockerfile instructions are executed. It can be set to any location within the image.\n",
        " - E.g. Working directory: WORKDIR /app\n",
        " \n",
        "Using a Dockerfile provides a consistent and repeatable way to build Docker images, making it easier to manage and deploy containers. By defining the image creation process in a Dockerfile, developers can easily version control their image builds and ensure that the same image is built every time."
      ],
      "metadata": {
        "id": "Lq1mLF7N0_5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of a simple Dockerfile that creates a Docker image for a Python Flask web application:\n",
        "- The Dockerfile sets the base image to Python 3.9\n",
        "- Installs the dependencies from the requirements file\n",
        "- Copies the application code to the working directory\n",
        "- Sets the environment variables\n",
        "- Exposes port 5000 for the application\n",
        "- Runs the Flask web server"
      ],
      "metadata": {
        "id": "h7DcSsyA16sk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the base image\n",
        "FROM python:3.9\n",
        "\n",
        "# Set the working directory\n",
        "WORKDIR /app\n",
        "\n",
        "# Copy the requirements file\n",
        "COPY requirements.txt .\n",
        "\n",
        "# Install the dependencies\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "\n",
        "# Copy the application code\n",
        "COPY . .\n",
        "\n",
        "# Set the environment variables\n",
        "ENV FLASK_APP=app.py\n",
        "\n",
        "# Expose the application port\n",
        "EXPOSE 5000\n",
        "\n",
        "# Run the application\n",
        "CMD [\"flask\", \"run\", \"--host=0.0.0.0\"]"
      ],
      "metadata": {
        "id": "Bagtzi8v16sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 29. How do you build a Docker image using a Dockerfile?"
      ],
      "metadata": {
        "id": "M6bOj2sTkbha"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a Docker image using a Dockerfile:\n",
        "- Create a Dockerfile:\n",
        " - Create a text file called \"Dockerfile\" in a directory where you want to build the image.\n",
        "- Write the Dockerfile instructions:\n",
        " - In the Dockerfile, write the instructions for building the image.\n",
        " - These instructions typically start with a base image and include commands for installing packages, copying files, and configuring the environment.\n",
        "- Build the Docker image:\n",
        " - Run the \"docker build\" command in the directory where the Dockerfile is located. This command will read the instructions in the Dockerfile and build a Docker image based on those instructions."
      ],
      "metadata": {
        "id": "KJh678hX2_B6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The command builds a Docker image with the tag \"my-flask-app\" using the Dockerfile in the current directory (\".\") as the build context. Once the Docker image is built, it can be run as a Docker container on any platform that supports Docker.\n",
        "\n",
        "The \"-t\" flag is used to specify the name and tag for the image, and the \".\" indicates that the build context is the current directory."
      ],
      "metadata": {
        "id": "OfqrxM6G8Y7n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic syntax\n",
        "docker build -t <image_name> .\n",
        "\n",
        "# Example from above\n",
        "docker build -t my-flask-app ."
      ],
      "metadata": {
        "id": "r90hX3kQ2_B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 30. How can you share data between Docker containers?"
      ],
      "metadata": {
        "id": "L6tRA8MzlwOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several ways to share data between Docker containers:\n",
        "\n",
        "- Docker volumes (recommended method):\n",
        " - A volume is a persistent data storage mechanism that can be shared between multiple containers.\n",
        " - Volumes can be created using the \"docker volume create\" command, and then mounted to one or more containers using the \"-v\" flag.\n",
        " - Data written to the volume from one container can be read from another container that shares the same volume.\n",
        "\n",
        "- Docker networks:\n",
        " - Docker networks enable containers to communicate with each other, and data can be shared between containers through network connections.\n",
        " - Containers can be connected to the same network using the \"docker network connect\" command.\n",
        "\n",
        "- Shared file systems:\n",
        " - If multiple containers are running on the same host, they can share data using a shared file system.\n",
        " - In this approach, files are stored on the host file system and mounted into the containers using the \"-v\" flag.\n",
        "\n",
        "- Docker Compose:\n",
        " - Docker Compose is a tool for defining and running multi-container Docker applications.\n",
        " - With Docker Compose, you can define multiple services (containers) that interact with each other and share data through volumes or networks."
      ],
      "metadata": {
        "id": "TlXUBlHK9Xpb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting a volume to a Docker container allows you to persist data across container restarts and share data between containers. Docker supports two types of volumes: \n",
        "- Bind mounts\n",
        " - Bind mounts map a directory or file on the host system to a directory or file in the container. This method is useful when you need to access files on the host system or share data between the host and the container.\n",
        "- Managed volumes\n",
        " - Managed volumes are created and managed by Docker, providing better isolation and improved performance compared to bind mounts. Docker automatically manages the storage and lifecycle of these volumes.\n",
        "\n",
        "In both bind mounts and managed volumes, the volume is mounted at /path/to/container/directory in the container, and any changes made in the container's mounted directory will persist even after the container is stopped or removed."
      ],
      "metadata": {
        "id": "9Db9uejO97Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://techmormo.com/wp-content/uploads/2022/11/docker4-stateless-containers.png)"
      ],
      "metadata": {
        "id": "5JBck9lC93Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Refactoring"
      ],
      "metadata": {
        "id": "7hOy2zIdlwLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 31. How would you define Code Refactoring (CF)?"
      ],
      "metadata": {
        "id": "qIQMu3r3lwIO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definition:\n",
        "- CF is the process of restructuring existing code to improve its internal structure, without changing its external behaviour\n",
        "\n",
        "\n",
        "Goal:\n",
        "- To make the code easier to read,understand, and maintain as well as to improve it's performance, scalabiltiy and reliability\n",
        "\n",
        "Steps:\n",
        "- Transformation of 'working' code into deployed code\n",
        "- Separating parts of code into multiple files\n",
        "- Organizing repository to be deployment-ready"
      ],
      "metadata": {
        "id": "S0BBjjlb4Mc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. How does CF influence the external behavior of the code?"
      ],
      "metadata": {
        "id": "f9udqqMnlwFM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code refactoring should not change the external behavior of the code. The purpose of refactoring is to improve the internal structure and design of the code, without modifying its functionality or behavior."
      ],
      "metadata": {
        "id": "cpgw3mhm56OS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. What are the concrete goals one wants to achieve with CF?"
      ],
      "metadata": {
        "id": "yFnGiTSClwCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common goals of CF include:\n",
        "- Improving code readability: By improving the structure and organization of the code, it becomes easier for developers to understand and work with it.\n",
        "- Increasing code maintainability: CF can help simplify complex code, reduce dependencies, and eliminate redundant code, making it easier to maintain it.\n",
        "- Enhancing code reusability: CF can help extract common functionality into reusable components, reducing the amount of code that needs to be written and improving overall code efficiency.\n",
        "- Optimizing code performance: CF can help identify and eliminate bottlenecks and improve the overall performance.\n",
        "- Reducing technical debt: By addressing technical debt, or the accumulation of bad code practices over time, CF can help improve the overall quality and sustainability of the codebase.\n",
        "- Making the code more testable: CF can help reduce coupling and increase cohesion, making the code more modular and easier to test."
      ],
      "metadata": {
        "id": "xS25p0vG7Ecf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 32. What do we call code smell, and what are the most common types?"
      ],
      "metadata": {
        "id": "0K5UgZuYlv_B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- \"Code smell\" refers to potential design problems or poor programming practices in code.\n",
        "- Code smells are not necessarily bugs or errors, but they may make the code difficult to maintain, modify, or extend over time."
      ],
      "metadata": {
        "id": "24gg6w4q8GGo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common types of code smell includes:\n",
        "- Long functions\n",
        "- Duplicate code\n",
        "- Dead code\n",
        "- Data clumps\n",
        "- Improper names"
      ],
      "metadata": {
        "id": "esoJFV4w-qEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other types includes:\n",
        "- Large class\n",
        "- Feature envy\n",
        "- Primitive obsession\n",
        "- Shotgun surgery\n",
        "- God object"
      ],
      "metadata": {
        "id": "9Hkgtv2k-nma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Can you describe any of the types, why does it appear and how to solve/improve it?"
      ],
      "metadata": {
        "id": "p3ymREoGlv7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Common types:\n",
        "\n",
        "- Long functions\n",
        " - Description: code blocks that are too long and complex, containing too many lines of code, making them harder to read and understand.\n",
        " - Why it appears: devs try to solve too many problems at once or fail to break down a complex problem into smaller, more manageable parts\n",
        " - How to solve/improve it: break down complex problems into smaller, more manageable parts, and write separate functions to solve each part\n",
        "\n",
        "- Duplicate code\n",
        " - Description: code blocks that appear in multiple places in the codebase, leading to redundancy and increased risk of errors\n",
        " - Why it appears: devs copy/paste code instead of creating reusable functions or modules.\n",
        " - How to solve/improve it: create reusable functions or modules that can be called from multiple places in the codebase\n",
        "\n",
        "- Dead code\n",
        " - Description: code blocks that are no longer being used in the codebase and serve no purpose.\n",
        " - Why it appears: devs forget to remove code that has been replaced or is no longer needed.\n",
        " - How to solve/improve it: regularly review the codebase and remove any code blocks that are no longer being used or serve no purpose\n",
        "\n",
        "- Data clumps\n",
        " - Description: groups of data elements that appear together in multiple places throughout the codebase, leading to redundancy and increased complexity\n",
        " - Why it appears: devs fail to encapsulate related data elements into classes or data structures.\n",
        " - How to solve/improve it: group related data elements together into classes or data structures\n",
        " \n",
        "- Improper names\n",
        " - Description: unclear, confusing, or inconsistent naming conventions in the codebase, leading to reduced readability and increased confusion\n",
        " - Why it appears: devs fail to establish clear and consistent naming conventions in the codebase.\n",
        " - How to solve/improve it: establish clear and consistent naming conventions in the codebase. They should use descriptive, meaningful, and easy-to-understand names and avoid names that are too short, too long, or inconsistent with the established conventions"
      ],
      "metadata": {
        "id": "uYeDTaqe_c_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Other types:\n",
        "- Large class\n",
        " - Description: classes that contain too much functionality or data, leading to increased complexity and reduced maintainability\n",
        " - Why it appears: devs try to solve too many problems in a single class or fail to encapsulate related functionality into separate classes.\n",
        " - How to solve/improve it: break down complex functionality into smaller, more focused classes. They should use inheritance, composition, and other design patterns to create a modular and maintainable codebase.\n",
        "\n",
        "- Feature envy\n",
        " - Description: code blocks that use too much data or functionality from other classes, leading to increased coupling and reduced flexibility\n",
        " - Why it appears: devs fail to encapsulate related functionality into separate classes or use classes in an inappropriate way\n",
        " - How to solve/improve it: use proper encapsulation and modularity principles. They should avoid using data or functionality from other classes unless it is absolutely necessary and should instead rely on established interfaces and APIs\n",
        "\n",
        "- Primitive obsession\n",
        " - Description: overuse of primitive data types instead of more appropriate data structures or objects, leading to increased complexity and reduced maintainability\n",
        " - Why it appears: devs fail to create appropriate data structures or objects for their code, or when they try to optimize for performance at the expense of maintainability\n",
        " - How to solve/improve it: use appropriate data structures and objects that reflect the domain model of the code. They should avoid using primitive data types as objects and should instead create custom objects and data structures where appropriate\n",
        "\n",
        "- Shotgun surgery\n",
        " - Description: changes that require modifying multiple parts of the codebase, leading to increased complexity and reduced maintainability\n",
        " - Why it appears: devs fail to properly modularize their code or create proper dependencies between different parts of the codebase.\n",
        " - How to solve/improve it: create proper modularization and dependency structures in their code. They should use design patterns such as dependency injection and inversion of control to reduce coupling between different parts of the codebase and make changes easier to manage\n",
        "\n",
        "- God object\n",
        " - Description: classes that contain too much functionality and data, making them difficult to understand and maintain\n",
        " - Why it appears: devs fail to properly modularize their code or create appropriate abstractions for their functionality\n",
        " - How to solve/improve it: break down complex functionality into smaller, more focused classes. They should use design patterns such as composition and inheritance to create modular and maintainable code. They should also use appropriate abstractions and interfaces to separate concerns and reduce complexity"
      ],
      "metadata": {
        "id": "L6GO9Eqr_fQK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 33. How can we improve understanding in reusability of our code when publishing in a specific platform or in specific company?"
      ],
      "metadata": {
        "id": "7PgfZApelv5B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following practices can be applied:\n",
        "\n",
        "- Standardization:\n",
        " - Developing a set of coding guidelines and standards can help ensure that the code is written in a consistent and reusable manner.\n",
        "\n",
        "- Documentation:\n",
        " - Documenting the code, including comments, API documentation, and user guides, can help other developers understand how to use the code and how it fits into the overall architecture.\n",
        "\n",
        "- Code reuse analysis:\n",
        " - Analyzing the code to identify common patterns or functions that can be reused, e.g.:\n",
        "   - looking for duplicate code\n",
        "   - creating libraries or modules that can be used across projects\n",
        "   - developing code designed to be generic and reusable\n",
        "\n",
        "- Education and training:\n",
        " - Help devs understand the importance of code reusability and best practices for achieving it. \n",
        " - This can include training on coding standards, documentation practices, and code reuse analysis.\n",
        "\n",
        "- Collaboration:\n",
        " - Encouraging collaboration among devs can help improve understanding of code reusability.\n",
        " - This can include pair programming, code reviews, and working together on shared projects."
      ],
      "metadata": {
        "id": "gAFHMutjDQh7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# API"
      ],
      "metadata": {
        "id": "II9dmVD5lv2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![picture](https://res.cloudinary.com/dyd911kmh/image/upload/v1664210695/A_simple_API_architecture_design_f98bfad9ce.png)"
      ],
      "metadata": {
        "id": "ZYA2jio3BdSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 34. Can you define Application Programming Interface?"
      ],
      "metadata": {
        "id": "VDramcEHlvzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definitions:\n",
        "- It is a set of defined rules that enable different applications to communicate with each other.\n",
        "- It acts as an intermediary layer that processes data transfers between systems.\n",
        "\n",
        "Usability:\n",
        "- APIs simplify software development and innovation by enabling applications to exchange data and functionality easily and securely."
      ],
      "metadata": {
        "id": "TrS7ifs_BXbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 35. What types of API do we know based on their availability?"
      ],
      "metadata": {
        "id": "oQHWI7d4lvvq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Public API\n",
        " - is open and available for use by any outside developer or business. These are also called open or external APIs\n",
        "- Partner API\n",
        " - is only available to specifically selected and authorized outside developers or API consumers. It facilitates business-to-business activities\n",
        "- Private API\n",
        " - is intended only for use within the enterprise to connect systems and data within the business\n",
        "- Composite API\n",
        " - is a sequence of tasks that run synchronously as a result of the execution and not at the request of a task."
      ],
      "metadata": {
        "id": "XFxzQ5fyD1WO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Can you describe any of the types, how they differentiate from others, and can you tell one case you would use it?"
      ],
      "metadata": {
        "id": "4DMENaS0lvsQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Public API:\n",
        " - These APIs are available for public use and can be accessed by anyone.\n",
        " - They are typically offered by companies or organizations as a way to promote their services, products, or platforms.\n",
        " - For example the OpenWeatherMap API, which provides weather data for various locations around the world. It can be used by developers to integrate weather information into their applications.\n",
        "\n",
        "- Partner API:\n",
        " - These APIs are offered to partners or third-party developers for specific purposes.\n",
        " - They are typically used to integrate with a specific service or platform.\n",
        " - For example, a social media platform might offer an API to allow developers to integrate their applications with the platform and access user data.\n",
        "\n",
        "- Private API:\n",
        " - These APIs are used within an organization and are not accessible to the public.\n",
        " - They are typically used to facilitate communication between different systems or services within the organization.\n",
        " - For example, a company might use an internal API to allow different departments to share data and information.\n",
        "\n",
        "- Composite APIs:\n",
        " - These APIs are created by combining multiple APIs into a single interface.\n",
        " - They are used to simplify complex processes and provide a unified interface to developers.\n",
        " - For example, a travel booking website might use a composite API to integrate with multiple airlines and hotels.\n",
        "\n",
        "\n",
        "In summary:\n",
        "- Public APIs are useful for promoting services or products\n",
        "- Partner APIs are useful for integrating with specific services or platforms\n",
        "- Private APIs are useful for facilitating communication within an organization\n",
        "- Composite APIs are useful for simplifying complex processes"
      ],
      "metadata": {
        "id": "7FEn-M_4HVnv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 36. What are the types based on their protocol?"
      ],
      "metadata": {
        "id": "O0T4L8fclvpc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- REST (Representational State Transfer)\n",
        " - is a web services API and crucial for modern web applications\n",
        "\n",
        "- SOAP (Simple object access protocol)\n",
        " - is a well-established protocol but comes with strict rules, rigid standards\n",
        "\n",
        "- RPC (Remote Procedure Call protocol)\n",
        " - is the oldest and simplest type of API with a goal for the client to execute code on a server\n",
        "\n",
        "- Event-driven or asynchronous APIs\n",
        " - transmit information in quasi-real-time. The advantage is that it allows the source to send a response only when the information is new or has changed, useful for stock exchanges"
      ],
      "metadata": {
        "id": "zEJ3fGX9Gt_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Can you describe any of the types, how they differentiate from others, and can you tell one case you would use it?"
      ],
      "metadata": {
        "id": "2DLi9LgnlvmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Description and use case examples:\n",
        "\n",
        "- REST (Representational State Transfer):\n",
        " - REST is an architectural style for building APIs that uses HTTP requests to perform CRUD (Create, Read, Update, Delete) operations on resources.\n",
        " - RESTful APIs use common HTTP verbs like GET, POST, PUT, and DELETE to manipulate data. They also typically use JSON (JavaScript Object Notation) as the data format.\n",
        " - Use case: REST APIs are widely used for building web applications and mobile apps.\n",
        "   - For example, a social media app might use a REST API to fetch and display posts, comments, and likes.\n",
        "   - For example, Twitter's REST API allows developers to retrieve tweets, user profiles, and other data related to the platform's users and content.\n",
        "\n",
        "- SOAP (Simple Object Access Protocol):\n",
        " - SOAP is an XML-based messaging protocol for exchanging information between web services.\n",
        " - SOAP uses a set of well-defined rules for messaging and authentication, and typically uses HTTP or SMTP (Simple Mail Transfer Protocol) for transport.\n",
        " - Use case: SOAP APIs are often used in enterprise applications that require high security and reliability.\n",
        "   - For example, a bank might use a SOAP API to exchange sensitive financial information between different systems.\n",
        "   - For example, integrating two enterprise systems, such as an order management system and a shipping provider's system.\n",
        "\n",
        "- RPC (Remote Procedure Call):\n",
        " - RPC is a protocol for building distributed applications in which a client calls a remote procedure (function) on a server and receives the result.\n",
        " - RPC can use different transport protocols such as HTTP, TCP, or UDP.\n",
        " - Use case: RPC APIs are often used for building microservices-based architectures where different services need to communicate with each other.\n",
        "   - For example, an e-commerce website might use an RPC API to handle order processing and inventory management.\n",
        "   - For example, controlling a remote device or system, such as a robotic arm or a network router.\n",
        "\n",
        "- Event-driven or asynchronous APIs:\n",
        " - Event-driven APIs use a publish-subscribe model in which events (such as updates or changes) are sent to subscribers who have expressed interest in them.\n",
        " - Asynchronous APIs use non-blocking I/O to handle requests and responses, allowing multiple requests to be processed simultaneously.\n",
        " - Use case: Event-driven and asynchronous APIs are often used for building real-time applications like chat apps or online games.\n",
        "   - For example, a chat app might use an event-driven API to send and receive messages in real-time.\n",
        "   - For example, a real-time data streaming service, such as a stock market feed. The API would push updates to subscribed clients as new data becomes available, rather than requiring the clients to repeatedly query for updates."
      ],
      "metadata": {
        "id": "flktVLUNJeHq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How they differentiate:\n",
        "- Architecture:\n",
        " - REST and SOAP are based on the client-server model\n",
        " - RPC and event-driven APIs are based on the peer-to-peer model.\n",
        "\n",
        "- Data format:\n",
        " - REST typically uses JSON or XML to format data\n",
        " - SOAP uses XML\n",
        " - RPC uses various data formats such as JSON and XML\n",
        " - Event-driven APIs can use any data format.\n",
        "\n",
        "- Communication:\n",
        " - REST, SOAP, and RPC use synchronous communication (sender and receiver interact in real-time)\n",
        " - Event-driven APIs use asynchronous communication.\n",
        "\n",
        "- Transport protocol:\n",
        " - REST and SOAP use HTTP as the transport protocol\n",
        " - RPC can use any protocol\n",
        " - Event-driven APIs can use any protocol that supports asynchronous communication.\n",
        "\n",
        "- Endpoint definition:\n",
        " - REST and SOAP define endpoints using URLs\n",
        " - RPC and event-driven APIs use function calls or event subscriptions.\n",
        "\n",
        "- Caching:\n",
        " - REST has built-in support for caching\n",
        " - SOAP and RPC do not.\n",
        "\n",
        "- Error handling:\n",
        " - REST uses HTTP status codes for error handling\n",
        " - SOAP and RPC use custom error codes"
      ],
      "metadata": {
        "id": "HrtGWyZULyeb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 37. We use API options that can be passed with the endpoint to influence the response. How do we call them, and can you give a few examples?"
      ],
      "metadata": {
        "id": "qXctE_1alvi4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- API options can be passed as query parameters in the URL\n",
        "- To call an API option, you append the option name and value to the end of the URL using the \"?\" separator. Multiple options can be separated with ampersand \"&\".\n",
        "- Query parameters can be in the form of HTTP requests such as GET, POST, PUT, DELETE and PATCH\n",
        "- Some options require a python script using the 'requests' library to pass the request body to the URL\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gtKC9b5HNlHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are API options used for:\n",
        "\n",
        "- Filtering: You can filter data by passing parameters that specify certain conditions that must be met.\n",
        "\n",
        "- Sorting: You can sort data by passing parameters that specify which field to sort by and whether to sort in ascending or descending order.\n",
        "\n",
        "- Pagination: If an API endpoint returns a large amount of data, you can use pagination parameters to control how much data is returned at a time.\n",
        "\n",
        "- Authentication: You can pass authentication parameters to authenticate the API request.\n",
        "\n",
        "- Language: Some APIs allow you to specify the language of the response by passing a language parameter."
      ],
      "metadata": {
        "id": "Vxj_NwCCU0Km"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Can you describe any of the request types, and for what purpose do we use it?"
      ],
      "metadata": {
        "id": "WG_rjdbLlvfP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GET:\n",
        " - This request type is used to retrieve data from a server.\n",
        " - For example, if you want to retrieve information about a user from a database, you would use a GET request to retrieve that information."
      ],
      "metadata": {
        "id": "ysthtP11G0K_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- POST:\n",
        " - This request type is used to send data to a server to create or update a resource.\n",
        " - For example, if you want to create a new user in a database, you would use a POST request to send the user's information to the server."
      ],
      "metadata": {
        "id": "q1Ve5ss0WDoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PUT:\n",
        " - This request type is used to update an existing resource on the server.\n",
        " - For example, if you want to update a user's information in a database, you would use a PUT request to send the updated information to the server."
      ],
      "metadata": {
        "id": "KMeL0VEBWFRO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- DELETE:\n",
        " - This request type is used to delete a resource from the server.\n",
        " - For example, if you want to delete a user from a database, you would use a DELETE request to remove that user's information from the server."
      ],
      "metadata": {
        "id": "yrKwIH2aWHon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PATCH:\n",
        " - This request type is used to make a partial update to an existing resource on the server.\n",
        " - For example, if you want to update only a specific field of a user's information in a database, you would use a PATCH request to send only the updated field to the server."
      ],
      "metadata": {
        "id": "GPELokYFVl0e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code example of POST option:"
      ],
      "metadata": {
        "id": "L3CxNJuYeDXk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of URL\n",
        "POST https://api.example.com/users"
      ],
      "metadata": {
        "id": "no32dU3lW6NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of request body\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"email\": \"johndoe@example.com\",\n",
        "  \"password\": \"secretpassword\"\n",
        "}"
      ],
      "metadata": {
        "id": "HzhkB9F3W7w6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of request in python, i.e. how the request body is POSTed to the URL\n",
        "import requests\n",
        "import json\n",
        "\n",
        "url = 'https://example.com/api/users'\n",
        "\n",
        "data = {\n",
        "    'name': 'John Doe',\n",
        "    'email': 'john@example.com',\n",
        "    'password': '12345'\n",
        "}\n",
        "\n",
        "headers = {\n",
        "    'Content-Type': 'application/json'\n",
        "}\n",
        "\n",
        "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
        "\n",
        "print(response.status_code)\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "id": "gz-wzfOYYaoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 38. Discuss pro/cons of using API deployement on a virtual machine vs a managed serverless solution (e.g. GCP CloudRun)"
      ],
      "metadata": {
        "id": "Zlbsu44BlvcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Virtual Machine (VM)\n",
        "\n",
        "Pros:\n",
        "\n",
        "- More control: With a VM, you have complete control over the software stack, including the operating system and any libraries or dependencies that are required for your API.\n",
        "- Customizable infrastructure: You can customize the infrastructure to your exact needs, such as storage, networking, and security.\n",
        "- Flexibility: A VM can be used to deploy a wide range of applications, including APIs, web applications, and databases.\n",
        "\n",
        "Cons:\n",
        "\n",
        "- Maintenance: Managing a VM requires ongoing maintenance, including security updates, software updates, and patches.\n",
        "- Scalability: Scaling a VM can be complex, as it requires additional resources and configuration.\n",
        "- Cost: VMs can be expensive to run, especially if you require a high level of resources or require custom infrastructure."
      ],
      "metadata": {
        "id": "_fS9gclBeev8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Managed Serverless Solution\n",
        "\n",
        "Pros:\n",
        "\n",
        "- Easy deployment: Managed serverless solutions make it easy to deploy and run your API, as they handle the underlying infrastructure for you.\n",
        "- Automatic scaling: These solutions can automatically scale to handle increases in traffic, which can be beneficial for APIs that experience fluctuating traffic.\n",
        "- Low maintenance: With a serverless solution, you don't need to worry about managing the underlying infrastructure, as this is taken care of for you.\n",
        "- Cost-effective: You only pay for the resources you use, so serverless solutions can be cost-effective for APIs with low to moderate traffic.\n",
        "\n",
        "Cons:\n",
        "\n",
        "- Less control: With a serverless solution, you have less control over the underlying infrastructure, which can be a problem if you require specific configurations.\n",
        "- Performance: Serverless solutions can be slower than running an API on a dedicated VM, as the infrastructure needs time to spin up when a request is made.\n",
        "- Vendor lock-in: You may be locked into a specific vendor's platform if you choose to use a serverless solution, which can be problematic if you want to switch to a different provider."
      ],
      "metadata": {
        "id": "fDjZGJ1PeswP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary:\n",
        "\n",
        "Virtual machine: If you require a high level of control and customization or have specific requirements\n",
        "\n",
        "Managed serverless solution: if you want a cost-effective, easy-to-deploy solution with automatic scaling"
      ],
      "metadata": {
        "id": "1hMD1cjce7xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fast API"
      ],
      "metadata": {
        "id": "i501R9Z4hK2A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastAPI\n",
        "\n",
        "FastAPI is a modern, fast (high-performance) web framework for building APIs with Python 3.7+.\n",
        "- It is fast\n",
        "- Supports asynchronous code (async and await commands). It can perform multiple tasks concurrently. In that way, it doesnt need to wait until one called to be answered and can continue with a new request\n",
        "- Short development line 7-8 lines of code\n",
        "\n",
        "\n",
        "Uvicorn + FastAPI\n",
        "- Uvicorn is a lightning-fast ASGI server that is used to serve FastAPI applications.\n",
        "- FastAPI is built on top of Starlette which itself is built on top of Uvicorn."
      ],
      "metadata": {
        "id": "gtDVecqthMxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tranformation process\n",
        "\n",
        "Starting point: A folder with working python code for prediction on already trained model saved in sub-folder.\n",
        "Steps:\n",
        "- Install dependencies: fastapi, uvicorn, pydentic\n",
        "- Create a new python script based on prediction script which initiate fastapi app\n",
        "- define request parameters and their types\n",
        "- start uvicorn server and define address and fastapi app \n",
        "- Send request and hope for response 200 :)"
      ],
      "metadata": {
        "id": "F1tkP7kRhS_H"
      }
    }
  ]
}